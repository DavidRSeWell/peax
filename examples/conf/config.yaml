# Reference configuration for CleanRL DQN Self-Play training
# NOTE: This file is NOT loaded by default. The script uses the dataclass defaults.
# To use this file: python examples/cleanrl_dqn.py --config-path=conf --config-name=config
# Override individual values via command line: python examples/cleanrl_dqn.py learning_rate=1e-4

# Experiment settings
exp_name: cleanrl_dqn_peax_selfplay
seed: 1

# Algorithm hyperparameters (optimized for stability)
total_timesteps: 500000
learning_rate: 1.0e-4  # Reduced for stability
buffer_size: 50000  # Increased to reduce overfitting
gamma: 0.9  # Reduced to limit value accumulation
tau: 1.0
target_network_frequency: 1000  # Increased for stability
batch_size: 128
max_grad_norm: 1.0  # Gradient clipping
reward_clip: 2.0  # Reward clipping

# Exploration settings
start_e: 1.0
end_e: 0.2  # Increased for more exploration
exploration_fraction: 0.5
learning_starts: 10000
train_frequency: 10
eval_every: 10000

# Environment settings
boundary_type: square  # Options: square, circle, triangle
boundary_size: 10.0
max_steps: 200
capture_radius: 0.5
num_actions_per_dim: 5  # Total actions = this^2 (e.g., 5x5 = 25 actions)

# Reward shaping (to discourage wall-sitting)
wall_penalty_coef: 0.01  # Penalty for being near walls
velocity_reward_coef: 0.005  # Reward for movement
